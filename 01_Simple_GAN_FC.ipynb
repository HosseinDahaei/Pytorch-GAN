{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Simple GAN using fully connected layers\n",
    "for generating digit '4'\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_dim):\n",
    "        super().__init__()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(256, img_dim),\n",
    "            nn.Tanh(),  # normalize inputs to [-1, 1] so make outputs [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters etc.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 0.0001\n",
    "z_dim = 64\n",
    "image_dim = 28 * 28 * 1  # 784\n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "\n",
    "discriminator = Discriminator(image_dim).to(device)\n",
    "generator = Generator(z_dim, image_dim).to(device)\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)\n",
    "_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = datasets.MNIST(root=\"data/\", transform=_transforms, download=True)\n",
    "# Assuming 'data' is the tensor containing the images\n",
    "# Assuming 'dataset' is the MNIST dataset\n",
    "\n",
    "# Get the labels of the dataset\n",
    "labels = dataset.targets.numpy()\n",
    "\n",
    "# Find the indices of images with label 4\n",
    "indices = np.where(labels == 4)[0]\n",
    "dataset_4 = torch.utils.data.Subset(dataset, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset_4, batch_size=batch_size, shuffle=True)\n",
    "opt_discriminator = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "opt_generator = optim.Adam(generator.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100] Batch 0/92                       Loss D: 0.6722, loss G: 0.6795\n",
      "Epoch [1/100] Batch 0/92                       Loss D: 0.4701, loss G: 0.6112\n",
      "Epoch [2/100] Batch 0/92                       Loss D: 0.5909, loss G: 0.5065\n",
      "Epoch [3/100] Batch 0/92                       Loss D: 0.4675, loss G: 0.7484\n",
      "Epoch [4/100] Batch 0/92                       Loss D: 0.4163, loss G: 0.8406\n",
      "Epoch [5/100] Batch 0/92                       Loss D: 0.4951, loss G: 0.7173\n",
      "Epoch [6/100] Batch 0/92                       Loss D: 0.5402, loss G: 0.7000\n",
      "Epoch [7/100] Batch 0/92                       Loss D: 0.5857, loss G: 0.6309\n",
      "Epoch [8/100] Batch 0/92                       Loss D: 0.5054, loss G: 0.8126\n",
      "Epoch [9/100] Batch 0/92                       Loss D: 0.5596, loss G: 0.7124\n",
      "Epoch [10/100] Batch 0/92                       Loss D: 0.5355, loss G: 0.7587\n",
      "Epoch [11/100] Batch 0/92                       Loss D: 0.6342, loss G: 0.6778\n",
      "Epoch [12/100] Batch 0/92                       Loss D: 0.5564, loss G: 0.7584\n",
      "Epoch [13/100] Batch 0/92                       Loss D: 0.5993, loss G: 0.7124\n",
      "Epoch [14/100] Batch 0/92                       Loss D: 0.5910, loss G: 0.7456\n",
      "Epoch [15/100] Batch 0/92                       Loss D: 0.4119, loss G: 1.0803\n",
      "Epoch [16/100] Batch 0/92                       Loss D: 0.5353, loss G: 0.8729\n",
      "Epoch [17/100] Batch 0/92                       Loss D: 0.3768, loss G: 1.1296\n",
      "Epoch [18/100] Batch 0/92                       Loss D: 0.5492, loss G: 0.8336\n",
      "Epoch [19/100] Batch 0/92                       Loss D: 0.3191, loss G: 1.3093\n",
      "Epoch [20/100] Batch 0/92                       Loss D: 0.5528, loss G: 0.8256\n",
      "Epoch [21/100] Batch 0/92                       Loss D: 0.4850, loss G: 0.9445\n",
      "Epoch [22/100] Batch 0/92                       Loss D: 0.5522, loss G: 0.8575\n",
      "Epoch [23/100] Batch 0/92                       Loss D: 0.3886, loss G: 1.1023\n",
      "Epoch [24/100] Batch 0/92                       Loss D: 0.5755, loss G: 0.8065\n",
      "Epoch [25/100] Batch 0/92                       Loss D: 0.5104, loss G: 0.9142\n",
      "Epoch [26/100] Batch 0/92                       Loss D: 0.5995, loss G: 0.8718\n",
      "Epoch [27/100] Batch 0/92                       Loss D: 0.6255, loss G: 0.7418\n",
      "Epoch [28/100] Batch 0/92                       Loss D: 0.5684, loss G: 0.8153\n",
      "Epoch [29/100] Batch 0/92                       Loss D: 0.4621, loss G: 1.0093\n",
      "Epoch [30/100] Batch 0/92                       Loss D: 0.4907, loss G: 0.9954\n",
      "Epoch [31/100] Batch 0/92                       Loss D: 0.7024, loss G: 0.6660\n",
      "Epoch [32/100] Batch 0/92                       Loss D: 0.6082, loss G: 0.8034\n",
      "Epoch [33/100] Batch 0/92                       Loss D: 0.4553, loss G: 1.0139\n",
      "Epoch [34/100] Batch 0/92                       Loss D: 0.5793, loss G: 0.8194\n",
      "Epoch [35/100] Batch 0/92                       Loss D: 0.5773, loss G: 0.8717\n",
      "Epoch [36/100] Batch 0/92                       Loss D: 0.5374, loss G: 0.8714\n",
      "Epoch [37/100] Batch 0/92                       Loss D: 0.5167, loss G: 0.9492\n",
      "Epoch [38/100] Batch 0/92                       Loss D: 0.5612, loss G: 0.8795\n",
      "Epoch [39/100] Batch 0/92                       Loss D: 0.5711, loss G: 0.9061\n",
      "Epoch [40/100] Batch 0/92                       Loss D: 0.5424, loss G: 0.9335\n",
      "Epoch [41/100] Batch 0/92                       Loss D: 0.5166, loss G: 0.9361\n",
      "Epoch [42/100] Batch 0/92                       Loss D: 0.5214, loss G: 0.9297\n",
      "Epoch [43/100] Batch 0/92                       Loss D: 0.3951, loss G: 1.1730\n",
      "Epoch [44/100] Batch 0/92                       Loss D: 0.2870, loss G: 1.5195\n",
      "Epoch [45/100] Batch 0/92                       Loss D: 0.3966, loss G: 1.1485\n",
      "Epoch [46/100] Batch 0/92                       Loss D: 0.3370, loss G: 1.2479\n",
      "Epoch [47/100] Batch 0/92                       Loss D: 0.4941, loss G: 0.9101\n",
      "Epoch [48/100] Batch 0/92                       Loss D: 0.5265, loss G: 0.9158\n",
      "Epoch [49/100] Batch 0/92                       Loss D: 0.5906, loss G: 0.8445\n",
      "Epoch [50/100] Batch 0/92                       Loss D: 0.5734, loss G: 0.8138\n",
      "Epoch [51/100] Batch 0/92                       Loss D: 0.5901, loss G: 0.8376\n",
      "Epoch [52/100] Batch 0/92                       Loss D: 0.5477, loss G: 0.9116\n",
      "Epoch [53/100] Batch 0/92                       Loss D: 0.5622, loss G: 0.8474\n",
      "Epoch [54/100] Batch 0/92                       Loss D: 0.6249, loss G: 0.7920\n",
      "Epoch [55/100] Batch 0/92                       Loss D: 0.5249, loss G: 0.9293\n",
      "Epoch [56/100] Batch 0/92                       Loss D: 0.5055, loss G: 0.9948\n",
      "Epoch [57/100] Batch 0/92                       Loss D: 0.6771, loss G: 0.7788\n",
      "Epoch [58/100] Batch 0/92                       Loss D: 0.7101, loss G: 0.7249\n",
      "Epoch [59/100] Batch 0/92                       Loss D: 0.6748, loss G: 0.7692\n",
      "Epoch [60/100] Batch 0/92                       Loss D: 0.5609, loss G: 0.8754\n",
      "Epoch [61/100] Batch 0/92                       Loss D: 0.6337, loss G: 0.8218\n",
      "Epoch [62/100] Batch 0/92                       Loss D: 0.4860, loss G: 1.0088\n",
      "Epoch [63/100] Batch 0/92                       Loss D: 0.4741, loss G: 1.0952\n",
      "Epoch [64/100] Batch 0/92                       Loss D: 0.5020, loss G: 1.0454\n",
      "Epoch [65/100] Batch 0/92                       Loss D: 0.4023, loss G: 1.1971\n",
      "Epoch [66/100] Batch 0/92                       Loss D: 0.4715, loss G: 1.0803\n",
      "Epoch [67/100] Batch 0/92                       Loss D: 0.3364, loss G: 1.3006\n",
      "Epoch [68/100] Batch 0/92                       Loss D: 0.3558, loss G: 1.2821\n",
      "Epoch [69/100] Batch 0/92                       Loss D: 0.2708, loss G: 1.5662\n",
      "Epoch [70/100] Batch 0/92                       Loss D: 0.2307, loss G: 1.6358\n",
      "Epoch [71/100] Batch 0/92                       Loss D: 0.2813, loss G: 1.4707\n",
      "Epoch [72/100] Batch 0/92                       Loss D: 0.2301, loss G: 1.6351\n",
      "Epoch [73/100] Batch 0/92                       Loss D: 0.2865, loss G: 1.4498\n",
      "Epoch [74/100] Batch 0/92                       Loss D: 0.2480, loss G: 1.5354\n",
      "Epoch [75/100] Batch 0/92                       Loss D: 0.3625, loss G: 1.2556\n",
      "Epoch [76/100] Batch 0/92                       Loss D: 0.4077, loss G: 1.1330\n",
      "Epoch [77/100] Batch 0/92                       Loss D: 0.4591, loss G: 1.1011\n",
      "Epoch [78/100] Batch 0/92                       Loss D: 0.6275, loss G: 0.8526\n",
      "Epoch [79/100] Batch 0/92                       Loss D: 0.5206, loss G: 0.9718\n",
      "Epoch [80/100] Batch 0/92                       Loss D: 0.5260, loss G: 0.9630\n",
      "Epoch [81/100] Batch 0/92                       Loss D: 0.6318, loss G: 0.7806\n",
      "Epoch [82/100] Batch 0/92                       Loss D: 0.6766, loss G: 0.7621\n",
      "Epoch [83/100] Batch 0/92                       Loss D: 0.6487, loss G: 0.8107\n",
      "Epoch [84/100] Batch 0/92                       Loss D: 0.6099, loss G: 0.8754\n",
      "Epoch [85/100] Batch 0/92                       Loss D: 0.6798, loss G: 0.7621\n",
      "Epoch [86/100] Batch 0/92                       Loss D: 0.7100, loss G: 0.7213\n",
      "Epoch [87/100] Batch 0/92                       Loss D: 0.6210, loss G: 0.8405\n",
      "Epoch [88/100] Batch 0/92                       Loss D: 0.5635, loss G: 0.9304\n",
      "Epoch [89/100] Batch 0/92                       Loss D: 0.6028, loss G: 0.8826\n",
      "Epoch [90/100] Batch 0/92                       Loss D: 0.6649, loss G: 0.8207\n",
      "Epoch [91/100] Batch 0/92                       Loss D: 0.6019, loss G: 0.8891\n",
      "Epoch [92/100] Batch 0/92                       Loss D: 0.6018, loss G: 0.8650\n",
      "Epoch [93/100] Batch 0/92                       Loss D: 0.4913, loss G: 1.0447\n",
      "Epoch [94/100] Batch 0/92                       Loss D: 0.5832, loss G: 0.9322\n",
      "Epoch [95/100] Batch 0/92                       Loss D: 0.7044, loss G: 0.7819\n",
      "Epoch [96/100] Batch 0/92                       Loss D: 0.6621, loss G: 0.8151\n",
      "Epoch [97/100] Batch 0/92                       Loss D: 0.5480, loss G: 1.0127\n",
      "Epoch [98/100] Batch 0/92                       Loss D: 0.4336, loss G: 1.2306\n",
      "Epoch [99/100] Batch 0/92                       Loss D: 0.5373, loss G: 0.9604\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        # print(\"line 4: \",batch_idx,real.shape,_.shape)\n",
    "        real = real.view(-1, 784).to(device)\n",
    "        batch_size = real.shape[0]\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        noise = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake = generator(noise)\n",
    "        disc_real = discriminator(real).view(-1)\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = discriminator(fake).view(-1)\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        lossD = (lossD_real + lossD_fake) / 2\n",
    "        discriminator.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_discriminator.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        # where the second option of maximizing doesn't suffer from\n",
    "        # saturating gradients\n",
    "        output = discriminator(fake).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "        generator.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_generator.step()\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                      Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
    "            )\n",
    "            if (step) % 10 == 0:\n",
    "                with torch.no_grad():\n",
    "                    fake = generator(fixed_noise).reshape(-1, 1, 28, 28)\n",
    "                    data = real.reshape(-1, 1, 28, 28)\n",
    "                    img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                    img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "\n",
    "                    # Save fake image as PNG\n",
    "                    # if floder not exist, create it\n",
    "                    import os\n",
    "                \n",
    "                    if not os.path.exists(\"samples/Simple_GAN_FC/fake\"):\n",
    "                        os.makedirs(\"samples/Simple_GAN_FC/fake\")\n",
    "                    if not os.path.exists(\"samples/Simple_GAN_FC/real\"):\n",
    "                        os.makedirs(\"samples/Simple_GAN_FC/real\")\n",
    "                    vutils.save_image(\n",
    "                        img_grid_fake, f\"samples/Simple_GAN_FC/fake/{epoch}.png\", normalize=True\n",
    "                    )\n",
    "                    vutils.save_image(\n",
    "                        img_grid_real, f\"samples/Simple_GAN_FC/real/{epoch}.png\", normalize=True\n",
    "                    )\n",
    "\n",
    "                step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
