{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Simple GAN using fully connected layers\n",
    "for generating digit '4'\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_dim):\n",
    "        super().__init__()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(256, img_dim),\n",
    "            nn.Tanh(),  # normalize inputs to [-1, 1] so make outputs [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters etc.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 0.0001\n",
    "z_dim = 64\n",
    "image_dim = 28 * 28 * 1  # 784\n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "\n",
    "discriminator = Discriminator(image_dim).to(device)\n",
    "generator = Generator(z_dim, image_dim).to(device)\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)\n",
    "_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = datasets.MNIST(root=\"data/\", transform=_transforms, download=True)\n",
    "# Assuming 'data' is the tensor containing the images\n",
    "# Assuming 'dataset' is the MNIST dataset\n",
    "\n",
    "# Get the labels of the dataset\n",
    "labels = dataset.targets.numpy()\n",
    "\n",
    "# Find the indices of images with label 4\n",
    "indices = np.where(labels == 4)[0]\n",
    "dataset_4 = torch.utils.data.Subset(dataset, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset_4, batch_size=batch_size, shuffle=True)\n",
    "opt_discriminator = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "opt_generator = optim.Adam(generator.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100] Batch 0/92                       Loss D: 0.6861, loss G: 0.6670\n",
      "Epoch [1/100] Batch 0/92                       Loss D: 0.4625, loss G: 0.6171\n",
      "Epoch [2/100] Batch 0/92                       Loss D: 0.5991, loss G: 0.4932\n",
      "Epoch [3/100] Batch 0/92                       Loss D: 0.4579, loss G: 0.7643\n",
      "Epoch [4/100] Batch 0/92                       Loss D: 0.3861, loss G: 0.8797\n",
      "Epoch [5/100] Batch 0/92                       Loss D: 0.4599, loss G: 0.7778\n",
      "Epoch [6/100] Batch 0/92                       Loss D: 0.5188, loss G: 0.7155\n",
      "Epoch [7/100] Batch 0/92                       Loss D: 0.5704, loss G: 0.6544\n",
      "Epoch [8/100] Batch 0/92                       Loss D: 0.4817, loss G: 0.8202\n",
      "Epoch [9/100] Batch 0/92                       Loss D: 0.4946, loss G: 0.8374\n",
      "Epoch [10/100] Batch 0/92                       Loss D: 0.5184, loss G: 0.7857\n",
      "Epoch [11/100] Batch 0/92                       Loss D: 0.5742, loss G: 0.7267\n",
      "Epoch [12/100] Batch 0/92                       Loss D: 0.5264, loss G: 0.8096\n",
      "Epoch [13/100] Batch 0/92                       Loss D: 0.5075, loss G: 0.8792\n",
      "Epoch [14/100] Batch 0/92                       Loss D: 0.6030, loss G: 0.7232\n",
      "Epoch [15/100] Batch 0/92                       Loss D: 0.5322, loss G: 0.8507\n",
      "Epoch [16/100] Batch 0/92                       Loss D: 0.5278, loss G: 0.8747\n",
      "Epoch [17/100] Batch 0/92                       Loss D: 0.4359, loss G: 0.9963\n",
      "Epoch [18/100] Batch 0/92                       Loss D: 0.4976, loss G: 0.9410\n",
      "Epoch [19/100] Batch 0/92                       Loss D: 0.3674, loss G: 1.1918\n",
      "Epoch [20/100] Batch 0/92                       Loss D: 0.5385, loss G: 0.8573\n",
      "Epoch [21/100] Batch 0/92                       Loss D: 0.5280, loss G: 0.8570\n",
      "Epoch [22/100] Batch 0/92                       Loss D: 0.5432, loss G: 0.8275\n",
      "Epoch [23/100] Batch 0/92                       Loss D: 0.7205, loss G: 0.6429\n",
      "Epoch [24/100] Batch 0/92                       Loss D: 0.5200, loss G: 0.9037\n",
      "Epoch [25/100] Batch 0/92                       Loss D: 0.7112, loss G: 0.6510\n",
      "Epoch [26/100] Batch 0/92                       Loss D: 0.6055, loss G: 0.7852\n",
      "Epoch [27/100] Batch 0/92                       Loss D: 0.5153, loss G: 0.9077\n",
      "Epoch [28/100] Batch 0/92                       Loss D: 0.6705, loss G: 0.7174\n",
      "Epoch [29/100] Batch 0/92                       Loss D: 0.5216, loss G: 0.8675\n",
      "Epoch [30/100] Batch 0/92                       Loss D: 0.5762, loss G: 0.8288\n",
      "Epoch [31/100] Batch 0/92                       Loss D: 0.5378, loss G: 0.8825\n",
      "Epoch [32/100] Batch 0/92                       Loss D: 0.5858, loss G: 0.8360\n",
      "Epoch [33/100] Batch 0/92                       Loss D: 0.5840, loss G: 0.7783\n",
      "Epoch [34/100] Batch 0/92                       Loss D: 0.5985, loss G: 0.7806\n",
      "Epoch [35/100] Batch 0/92                       Loss D: 0.6960, loss G: 0.6848\n",
      "Epoch [36/100] Batch 0/92                       Loss D: 0.6674, loss G: 0.7395\n",
      "Epoch [37/100] Batch 0/92                       Loss D: 0.6421, loss G: 0.7534\n",
      "Epoch [38/100] Batch 0/92                       Loss D: 0.6243, loss G: 0.7900\n",
      "Epoch [39/100] Batch 0/92                       Loss D: 0.5853, loss G: 0.8394\n",
      "Epoch [40/100] Batch 0/92                       Loss D: 0.6164, loss G: 0.8288\n",
      "Epoch [41/100] Batch 0/92                       Loss D: 0.4230, loss G: 1.0934\n",
      "Epoch [42/100] Batch 0/92                       Loss D: 0.4690, loss G: 0.9524\n",
      "Epoch [43/100] Batch 0/92                       Loss D: 0.5289, loss G: 0.9764\n",
      "Epoch [44/100] Batch 0/92                       Loss D: 0.4662, loss G: 1.0355\n",
      "Epoch [45/100] Batch 0/92                       Loss D: 0.3857, loss G: 1.1638\n",
      "Epoch [46/100] Batch 0/92                       Loss D: 0.6064, loss G: 0.8449\n",
      "Epoch [47/100] Batch 0/92                       Loss D: 0.5506, loss G: 0.9481\n",
      "Epoch [48/100] Batch 0/92                       Loss D: 0.4098, loss G: 1.1373\n",
      "Epoch [49/100] Batch 0/92                       Loss D: 0.6218, loss G: 0.8131\n",
      "Epoch [50/100] Batch 0/92                       Loss D: 0.5765, loss G: 0.8643\n",
      "Epoch [51/100] Batch 0/92                       Loss D: 0.6362, loss G: 0.7777\n",
      "Epoch [52/100] Batch 0/92                       Loss D: 0.6375, loss G: 0.7901\n",
      "Epoch [53/100] Batch 0/92                       Loss D: 0.5715, loss G: 0.8699\n",
      "Epoch [54/100] Batch 0/92                       Loss D: 0.7276, loss G: 0.7087\n",
      "Epoch [55/100] Batch 0/92                       Loss D: 0.3849, loss G: 1.1690\n",
      "Epoch [56/100] Batch 0/92                       Loss D: 0.5787, loss G: 0.8292\n",
      "Epoch [57/100] Batch 0/92                       Loss D: 0.6214, loss G: 0.7684\n",
      "Epoch [58/100] Batch 0/92                       Loss D: 0.6158, loss G: 0.7900\n",
      "Epoch [59/100] Batch 0/92                       Loss D: 0.5531, loss G: 0.9480\n",
      "Epoch [60/100] Batch 0/92                       Loss D: 0.4418, loss G: 1.0800\n",
      "Epoch [61/100] Batch 0/92                       Loss D: 0.5417, loss G: 0.9444\n",
      "Epoch [62/100] Batch 0/92                       Loss D: 0.5137, loss G: 1.0206\n",
      "Epoch [63/100] Batch 0/92                       Loss D: 0.4346, loss G: 1.1542\n",
      "Epoch [64/100] Batch 0/92                       Loss D: 0.3945, loss G: 1.2095\n",
      "Epoch [65/100] Batch 0/92                       Loss D: 0.3979, loss G: 1.2127\n",
      "Epoch [66/100] Batch 0/92                       Loss D: 0.2375, loss G: 1.7105\n",
      "Epoch [67/100] Batch 0/92                       Loss D: 0.2253, loss G: 1.7152\n",
      "Epoch [68/100] Batch 0/92                       Loss D: 0.2745, loss G: 1.5250\n",
      "Epoch [69/100] Batch 0/92                       Loss D: 0.2428, loss G: 1.5915\n",
      "Epoch [70/100] Batch 0/92                       Loss D: 0.3313, loss G: 1.3338\n",
      "Epoch [71/100] Batch 0/92                       Loss D: 0.2546, loss G: 1.6454\n",
      "Epoch [72/100] Batch 0/92                       Loss D: 0.6002, loss G: 0.8533\n",
      "Epoch [73/100] Batch 0/92                       Loss D: 0.4653, loss G: 0.9602\n",
      "Epoch [74/100] Batch 0/92                       Loss D: 0.3478, loss G: 1.2679\n",
      "Epoch [75/100] Batch 0/92                       Loss D: 0.6527, loss G: 0.7827\n",
      "Epoch [76/100] Batch 0/92                       Loss D: 0.6146, loss G: 0.8473\n",
      "Epoch [77/100] Batch 0/92                       Loss D: 0.5307, loss G: 0.8977\n",
      "Epoch [78/100] Batch 0/92                       Loss D: 0.7727, loss G: 0.6976\n",
      "Epoch [79/100] Batch 0/92                       Loss D: 0.6400, loss G: 0.8315\n",
      "Epoch [80/100] Batch 0/92                       Loss D: 0.6153, loss G: 0.8686\n",
      "Epoch [81/100] Batch 0/92                       Loss D: 0.6421, loss G: 0.7546\n",
      "Epoch [82/100] Batch 0/92                       Loss D: 0.5780, loss G: 0.8950\n",
      "Epoch [83/100] Batch 0/92                       Loss D: 0.6843, loss G: 0.8120\n",
      "Epoch [84/100] Batch 0/92                       Loss D: 0.5912, loss G: 0.9000\n",
      "Epoch [85/100] Batch 0/92                       Loss D: 0.5503, loss G: 0.9326\n",
      "Epoch [86/100] Batch 0/92                       Loss D: 0.6863, loss G: 0.7839\n",
      "Epoch [87/100] Batch 0/92                       Loss D: 0.7325, loss G: 0.7559\n",
      "Epoch [88/100] Batch 0/92                       Loss D: 0.6991, loss G: 0.8189\n",
      "Epoch [89/100] Batch 0/92                       Loss D: 0.5792, loss G: 0.8840\n",
      "Epoch [90/100] Batch 0/92                       Loss D: 0.5916, loss G: 0.9440\n",
      "Epoch [91/100] Batch 0/92                       Loss D: 0.6473, loss G: 0.8068\n",
      "Epoch [92/100] Batch 0/92                       Loss D: 0.5335, loss G: 0.9976\n",
      "Epoch [93/100] Batch 0/92                       Loss D: 0.5653, loss G: 1.0028\n",
      "Epoch [94/100] Batch 0/92                       Loss D: 0.3832, loss G: 1.3649\n",
      "Epoch [95/100] Batch 0/92                       Loss D: 0.5311, loss G: 0.9831\n",
      "Epoch [96/100] Batch 0/92                       Loss D: 0.4448, loss G: 1.1281\n",
      "Epoch [97/100] Batch 0/92                       Loss D: 0.4497, loss G: 1.0946\n",
      "Epoch [98/100] Batch 0/92                       Loss D: 0.4452, loss G: 1.1900\n",
      "Epoch [99/100] Batch 0/92                       Loss D: 0.5149, loss G: 1.0385\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        # print(\"line 4: \",batch_idx,real.shape,_.shape)\n",
    "        real = real.view(-1, 784).to(device)\n",
    "        batch_size = real.shape[0]\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        noise = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake = generator(noise)\n",
    "        disc_real = discriminator(real).view(-1)\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = discriminator(fake).view(-1)\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        lossD = (lossD_real + lossD_fake) / 2\n",
    "        discriminator.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_discriminator.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        # where the second option of maximizing doesn't suffer from\n",
    "        # saturating gradients\n",
    "        output = discriminator(fake).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "        generator.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_generator.step()\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                      Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
    "            )\n",
    "            if (step) % 10 == 0:\n",
    "                with torch.no_grad():\n",
    "                    fake = generator(fixed_noise).reshape(-1, 1, 28, 28)\n",
    "                    data = real.reshape(-1, 1, 28, 28)\n",
    "                    img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                    img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "\n",
    "                    # Save fake image as PNG\n",
    "                    # if floder not exist, create it\n",
    "                    import os\n",
    "                \n",
    "                    if not os.path.exists(\"samples/Simple_GAN_FC/fake\"):\n",
    "                        os.makedirs(\"samples/Simple_GAN_FC/fake\")\n",
    "                    if not os.path.exists(\"samples/Simple_GAN_FC/real\"):\n",
    "                        os.makedirs(\"samples/Simple_GAN_FC/real\")\n",
    "                    vutils.save_image(\n",
    "                        img_grid_fake, f\"samples/Simple_GAN_FC/fake/{epoch}.png\", normalize=True\n",
    "                    )\n",
    "                    vutils.save_image(\n",
    "                        img_grid_real, f\"samples/Simple_GAN_FC/real/{epoch}.png\", normalize=True\n",
    "                    )\n",
    "\n",
    "                step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, file_name='checkpoint.pth.tar'):\n",
    "    path = \"./models/Simple_GAN_FC/\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    torch.save(state, path+file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving params.\n",
    "save_checkpoint({'epoch': epoch + 1, 'state_dict':discriminator.state_dict(), 'optimizer' : opt_discriminator.state_dict()}, 'D_c.pth.tar')\n",
    "save_checkpoint({'epoch': epoch + 1, 'state_dict':generator.state_dict(), 'optimizer' : opt_generator.state_dict()}, 'G_c.pth.tar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
